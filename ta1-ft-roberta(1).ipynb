{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7692456,"sourceType":"datasetVersion","datasetId":4489439}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n# !pip install wandb\nclear_output()\n","metadata":{"id":"tETJPxKir6Sk","execution":{"iopub.status.busy":"2024-02-25T12:39:34.803305Z","iopub.execute_input":"2024-02-25T12:39:34.803574Z","iopub.status.idle":"2024-02-25T12:39:34.815309Z","shell.execute_reply.started":"2024-02-25T12:39:34.803549Z","shell.execute_reply":"2024-02-25T12:39:34.814594Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\n!pip install peft\n!pip install bs4\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:39:34.819452Z","iopub.execute_input":"2024-02-25T12:39:34.819747Z","iopub.status.idle":"2024-02-25T12:40:12.562694Z","shell.execute_reply.started":"2024-02-25T12:39:34.819725Z","shell.execute_reply":"2024-02-25T12:40:12.561692Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nimport datasets\nfrom transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom tqdm import tqdm\n# import wandb\nimport os\nimport re\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch\nfrom datasets import load_dataset\n# from trl import SFTTrainer\n\nfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertForSequenceClassification, AdamW\n\n# import spacy\n# import pandas as pd\n# import numpy as np\n# import nltk\n# from nltk.tokenize.toktok import ToktokTokenizer\n# import re\n# from bs4 import BeautifulSoup\n# from contractions import CONTRACTION_MAP\n# import unicodedata\n\n\n# import pandas as pd\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.model_selection import train_test_split\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.metrics import accuracy_score\n# from nltk.tokenize import word_tokenize\n# from nltk.corpus import stopwords\n# from nltk.stem import WordNetLemmatizer\n# from nltk import pos_tag\n# import string\n\n# tokenizer = ToktokTokenizer()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:40:12.564812Z","iopub.execute_input":"2024-02-25T12:40:12.565119Z","iopub.status.idle":"2024-02-25T12:40:32.182023Z","shell.execute_reply.started":"2024-02-25T12:40:12.565091Z","shell.execute_reply":"2024-02-25T12:40:32.181051Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-02-25 12:40:21.872572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-25 12:40:21.872666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-25 12:40:21.997567: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\ntrain_dataset = pd.read_csv('/kaggle/input/ta-assignment1/train.csv')\ntest_dataset = pd.read_csv('/kaggle/input/ta-assignment1/test.csv')\n\ntrain_dataset['label'] = [1 if x==\"positive\" else 0 for x in train_dataset['sentiment'] ]\ntest_dataset['label'] = [1 if x==\"positive\" else 0 for x in test_dataset['sentiment'] ]\n\ntrain_dataset = train_dataset.drop('sentiment', axis=1)\ntest_dataset = test_dataset.drop('sentiment', axis=1)\n\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text\n\n\n\ntest_dataset['review'] = test_dataset['review'].apply(strip_html_tags)\ntrain_dataset['review'] = train_dataset['review'].apply(strip_html_tags)\n\n\n\ntrain_texts, train_labels = train_dataset['review'], train_dataset['label']\ntest_texts, test_labels = test_dataset['review'], test_dataset['label']\n\ntrain_texts.reset_index(drop=True, inplace=True)\ntest_texts.reset_index(drop=True, inplace=True)\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n\ntrain_texts.reset_index(drop=True, inplace=True)\nval_texts.reset_index(drop=True, inplace=True)\ntrain_labels.reset_index(drop=True, inplace=True)\nval_labels.reset_index(drop=True, inplace=True)\n\nfrom transformers import AutoTokenizer, FlaxRobertaForSequenceClassification\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:40:32.183040Z","iopub.execute_input":"2024-02-25T12:40:32.183589Z","iopub.status.idle":"2024-02-25T12:40:43.103369Z","shell.execute_reply.started":"2024-02-25T12:40:32.183564Z","shell.execute_reply":"2024-02-25T12:40:43.102413Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/449927488.py:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(text, \"html.parser\")\n","output_type":"stream"}]},{"cell_type":"code","source":"hf_token = \"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\"\nfrom transformers import (\n    RobertaForSequenceClassification,\n    RobertaTokenizer,\n    RobertaModel,\n)\n\nmodel_name = \"roberta-base\"\ntokenizer = RobertaTokenizer.from_pretrained(model_name, do_lower_case=True)\n\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n\nimport torch\n\nclass IMDBdataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n        # Ensure all encodings and labels have consistent lengths\n        assert all(len(val) == len(self.labels) for val in self.encodings.values()), \"Encodings and labels lengths do not match.\"\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = IMDBdataset(train_encodings, train_labels)\ntest_dataset = IMDBdataset(test_encodings,test_labels)\nval_dataset = IMDBdataset(val_encodings, val_labels)\n\nfrom transformers import AutoTokenizer, FlaxRobertaForSequenceClassification\n\nmodel = RobertaForSequenceClassification.from_pretrained(\n        model_name, num_labels=2, output_attentions=False, output_hidden_states=False)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:40:43.105536Z","iopub.execute_input":"2024-02-25T12:40:43.105829Z","iopub.status.idle":"2024-02-25T12:42:58.617241Z","shell.execute_reply.started":"2024-02-25T12:40:43.105805Z","shell.execute_reply":"2024-02-25T12:42:58.616335Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e5089886c8d4f1ebf70b779b52d40fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f41038b02d574b61a9add0c2720bc7b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f44fcf903f4defb488f7dd0f8e0219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a5ef9a2fc70466ea9ad325854ac179e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e7cc95f71a4d2eab73ff33dd48c76a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a28e82818c424e9db474f2a00118a093"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=16, shuffle=False)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:42:58.618476Z","iopub.execute_input":"2024-02-25T12:42:58.618829Z","iopub.status.idle":"2024-02-25T12:42:58.624972Z","shell.execute_reply.started":"2024-02-25T12:42:58.618797Z","shell.execute_reply":"2024-02-25T12:42:58.623954Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:42:58.626147Z","iopub.execute_input":"2024-02-25T12:42:58.626502Z","iopub.status.idle":"2024-02-25T12:42:58.641009Z","shell.execute_reply.started":"2024-02-25T12:42:58.626471Z","shell.execute_reply":"2024-02-25T12:42:58.640177Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optim = AdamW(model.parameters(),lr=5e-5)\n\nfrom tqdm import tqdm\nimport time\n\n\ntotal_start_time = time.time()  # Start timing for the total training time\n\n\nfor epoch in range(3):\n    epoch_start_time = time.time()  # Start timing for the epoch\n\n    model.train()  # Set the model to training mode\n    train_loss = 0\n    for batch in tqdm(train_dataloader):\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n        train_loss += loss.item()\n    train_loss /= len(train_dataloader)  # Calculate the average loss over all training batches\n\n    # Validation phase\n    model.eval()  # Set the model to evaluation mode\n    val_loss = 0\n    with torch.no_grad():  # Do not compute gradient to speed up computation and reduce memory usage\n        for batch in val_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs[0]\n            val_loss += loss.item()\n    val_loss /= len(val_dataloader)  # Calculate the average loss over all validation batches\n\n    epoch_end_time = time.time()  # End timing for the epoch\n\n    # Print the losses and the time taken for the epoch\n    print(f\"Epoch {epoch+1}/{3}: Train loss = {train_loss:.4f}, Validation loss = {val_loss:.4f}\")\n    print(f\"Time taken for epoch {epoch+1}: {epoch_end_time - epoch_start_time:.2f} seconds\")\n\ntotal_end_time = time.time()  # End timing for the total training time\nprint(f\"Total training time: {total_end_time - total_start_time:.2f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:42:58.642300Z","iopub.execute_input":"2024-02-25T12:42:58.642621Z","iopub.status.idle":"2024-02-25T14:39:01.360840Z","shell.execute_reply.started":"2024-02-25T12:42:58.642589Z","shell.execute_reply":"2024-02-25T14:39:01.359749Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n100%|██████████| 1500/1500 [35:36<00:00,  1.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3: Train loss = 0.2478, Validation loss = 0.2217\nTime taken for epoch 1: 2318.84 seconds\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1500/1500 [35:39<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3: Train loss = 0.1558, Validation loss = 0.1844\nTime taken for epoch 2: 2322.00 seconds\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1500/1500 [35:39<00:00,  1.43s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3: Train loss = 0.1131, Validation loss = 0.1833\nTime taken for epoch 3: 2321.86 seconds\nTotal training time: 6962.70 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:39:01.362013Z","iopub.execute_input":"2024-02-25T14:39:01.362308Z","iopub.status.idle":"2024-02-25T14:39:02.374915Z","shell.execute_reply.started":"2024-02-25T14:39:01.362282Z","shell.execute_reply":"2024-02-25T14:39:02.373988Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"save_directory = \"/kaggle/working/TA1/ft_roberta\"","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:39:02.376433Z","iopub.execute_input":"2024-02-25T14:39:02.376757Z","iopub.status.idle":"2024-02-25T14:39:02.381495Z","shell.execute_reply.started":"2024-02-25T14:39:02.376728Z","shell.execute_reply":"2024-02-25T14:39:02.380555Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:50:29.664983Z","iopub.execute_input":"2024-02-25T14:50:29.665717Z","iopub.status.idle":"2024-02-25T14:50:30.844829Z","shell.execute_reply.started":"2024-02-25T14:50:29.665685Z","shell.execute_reply":"2024-02-25T14:50:30.844037Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\n\n!huggingface-cli login --token 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\n\nfrom huggingface_hub import HfApi\napi = HfApi()\napi.upload_folder(\n    folder_path=\"/kaggle/working/TA1/ft_roberta\",\n    repo_id=\"Bilal326/Roberta-imdb-sentiment\",\n    repo_type=\"model\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:54:31.150484Z","iopub.execute_input":"2024-02-25T14:54:31.151408Z","iopub.status.idle":"2024-02-25T14:54:45.353774Z","shell.execute_reply.started":"2024-02-25T14:54:31.151348Z","shell.execute_reply":"2024-02-25T14:54:45.352719Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"561ab53e43164526834666930a2588f1"}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Bilal326/Roberta-imdb-sentiment/commit/e5a399160011898a80573260f8693f2bf5a2a135', commit_message='Upload folder using huggingface_hub', commit_description='', oid='e5a399160011898a80573260f8693f2bf5a2a135', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TFAutoModel, AutoTokenizer\nfrom transformers import RobertaForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(save_directory)\nmodel = RobertaForSequenceClassification.from_pretrained(\n        save_directory, return_dict=False)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:54:57.981283Z","iopub.execute_input":"2024-02-25T14:54:57.982242Z","iopub.status.idle":"2024-02-25T14:54:58.636912Z","shell.execute_reply.started":"2024-02-25T14:54:57.982186Z","shell.execute_reply":"2024-02-25T14:54:58.636052Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nmodel.eval()  # Ensure the model is in evaluation mode\n\n# Store predictions and actual labels\npredictions = []\nactuals = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        # Move tensors to the same device as the model\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)  # Only needed if you're also evaluating performance\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n        # Assuming you're doing classification and want the highest probability class\n        logits = outputs[0]\n        predicted_labels = torch.argmax(logits, dim=1)\n        predictions.extend(predicted_labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:54:58.638484Z","iopub.execute_input":"2024-02-25T14:54:58.638763Z","iopub.status.idle":"2024-02-25T15:06:01.043716Z","shell.execute_reply.started":"2024-02-25T14:54:58.638741Z","shell.execute_reply":"2024-02-25T15:06:01.042719Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [11:02<00:00, 30.19it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"actual_labels = []\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        labels = batch['labels'].to(device)  # Assuming labels are on the same device\n        actual_labels.extend(labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:06:01.044805Z","iopub.execute_input":"2024-02-25T15:06:01.045060Z","iopub.status.idle":"2024-02-25T15:06:11.685496Z","shell.execute_reply.started":"2024-02-25T15:06:01.045038Z","shell.execute_reply":"2024-02-25T15:06:11.684554Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [00:10<00:00, 1881.11it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"correct_predictions = sum(p == a for p, a in zip(predictions, actual_labels))\naccuracy = correct_predictions / len(predictions)\nprint(f\"Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:06:11.687487Z","iopub.execute_input":"2024-02-25T15:06:11.687780Z","iopub.status.idle":"2024-02-25T15:06:11.700844Z","shell.execute_reply.started":"2024-02-25T15:06:11.687754Z","shell.execute_reply":"2024-02-25T15:06:11.699793Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Accuracy: 93.73%\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef\n\n# Ensure predictions and actual_labels are numpy arrays or compatible formats\nprecision = precision_score(actual_labels, predictions)\nrecall = recall_score(actual_labels, predictions)\nf1 = f1_score(actual_labels, predictions)\nconf_matrix = confusion_matrix(actual_labels, predictions)\nmcc = matthews_corrcoef(actual_labels, predictions)\n\n# ROC-AUC score requires probability scores of the positive class, which might need model.predict_proba() or equivalent\n# If your model outputs probabilities, you can use:\n# roc_auc = roc_auc_score(actual_labels, prediction_probabilities)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Matthews Correlation Coefficient: {mcc:.2f}\")\n# print(f\"ROC-AUC Score: {roc_auc:.2f}\")  # Uncomment if you have probability predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:06:11.701871Z","iopub.execute_input":"2024-02-25T15:06:11.702113Z","iopub.status.idle":"2024-02-25T15:06:11.806680Z","shell.execute_reply.started":"2024-02-25T15:06:11.702092Z","shell.execute_reply":"2024-02-25T15:06:11.805830Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Precision: 0.93\nRecall: 0.94\nF1 Score: 0.94\nConfusion Matrix:\n[[9276  659]\n [ 596 9469]]\nMatthews Correlation Coefficient: 0.87\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Raw Model","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nraw_model = RobertaForSequenceClassification.from_pretrained('roberta-base')\nraw_model = raw_model.to(device=device)\nraw_model.eval()\n\n\n# Store predictions and actual labels\nraw_predictions = []\nraw_actuals = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        # Move tensors to the same device as the model\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)  # Only needed if you're also evaluating performance\n\n        outputs = raw_model(input_ids, attention_mask=attention_mask)\n\n        # Assuming you're doing classification and want the highest probability class\n        logits = outputs[0]\n        raw_predicted_labels = torch.argmax(logits, dim=1)\n        raw_predictions.extend(raw_predicted_labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:06:11.807839Z","iopub.execute_input":"2024-02-25T15:06:11.808103Z","iopub.status.idle":"2024-02-25T15:17:04.762582Z","shell.execute_reply.started":"2024-02-25T15:06:11.808080Z","shell.execute_reply":"2024-02-25T15:17:04.761709Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 20000/20000 [10:52<00:00, 30.67it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_actual_labels = []\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader):\n        raw_labels = batch['labels'].to(device)  # Assuming labels are on the same device\n        raw_actual_labels.extend(raw_labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:17:04.763698Z","iopub.execute_input":"2024-02-25T15:17:04.763948Z","iopub.status.idle":"2024-02-25T15:17:15.329969Z","shell.execute_reply.started":"2024-02-25T15:17:04.763927Z","shell.execute_reply":"2024-02-25T15:17:15.329049Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [00:10<00:00, 1894.29it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_correct_predictions = sum(p == a for p, a in zip(raw_predictions, raw_actual_labels))\nraw_accuracy = raw_correct_predictions / len(raw_predictions)\nprint(f\"Accuracy: {raw_accuracy * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:17:15.331353Z","iopub.execute_input":"2024-02-25T15:17:15.331720Z","iopub.status.idle":"2024-02-25T15:17:15.345302Z","shell.execute_reply.started":"2024-02-25T15:17:15.331688Z","shell.execute_reply":"2024-02-25T15:17:15.344292Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Accuracy: 50.32%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ensure predictions and actual_labels are numpy arrays or compatible formats\nprecision = precision_score(raw_actual_labels, raw_predictions)\nrecall = recall_score(raw_actual_labels, raw_predictions)\nf1 = f1_score(raw_actual_labels, raw_predictions)\nconf_matrix = confusion_matrix(raw_actual_labels, raw_predictions)\nmcc = matthews_corrcoef(raw_actual_labels, raw_predictions)\n\n# ROC-AUC score requires probability scores of the positive class, which might need model.predict_proba() or equivalent\n# If your model outputs probabilities, you can use:\n# roc_auc = roc_auc_score(actual_labels, prediction_probabilities)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Matthews Correlation Coefficient: {mcc:.2f}\")\n# print(f\"ROC-AUC Score: {roc_auc:.2f}\")  # Uncomment if you have probability predictions\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T15:17:15.347517Z","iopub.execute_input":"2024-02-25T15:17:15.347787Z","iopub.status.idle":"2024-02-25T15:17:15.447405Z","shell.execute_reply.started":"2024-02-25T15:17:15.347765Z","shell.execute_reply":"2024-02-25T15:17:15.446564Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Precision: 0.50\nRecall: 1.00\nF1 Score: 0.67\nConfusion Matrix:\n[[    0  9935]\n [    0 10065]]\nMatthews Correlation Coefficient: 0.00\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:39:04.702073Z","iopub.status.idle":"2024-02-25T14:39:04.702435Z","shell.execute_reply.started":"2024-02-25T14:39:04.702245Z","shell.execute_reply":"2024-02-25T14:39:04.702259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:39:04.703953Z","iopub.status.idle":"2024-02-25T14:39:04.704420Z","shell.execute_reply.started":"2024-02-25T14:39:04.704166Z","shell.execute_reply":"2024-02-25T14:39:04.704186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login\n\nfrom huggingface_hub import HfApi\napi = HfApi()\napi.upload_folder(\n    folder_path=\"/content/drive/MyDrive/DL_Project/Abishek_Flynn\",\n    repo_id=\"Bilal326/Flynn_LoRA\",\n    repo_type=\"model\",\n)","metadata":{},"execution_count":null,"outputs":[]}]}